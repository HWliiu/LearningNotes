### GAN

[TOC]

![image-20210711204551048](GAN.assets/image-20210711204551048.png)

#### Vanilla GAN

##### Loss Function

$$
\min _{G} \max _{D} V(D, G)=\mathbb{E}_{\boldsymbol{x} \sim p_{\text {data }}(\boldsymbol{x})}[\log D(\boldsymbol{x})]+\mathbb{E}_{\boldsymbol{z} \sim p_{\boldsymbol{z}}(\boldsymbol{z})}[\log (1-D(G(\boldsymbol{z})))]
$$

è¿™ä¸ªæŸå¤±å‡½æ•°æŠŠåˆ¤åˆ«å™¨å’Œç”Ÿæˆå™¨çš„æŸå¤±å‡½æ•°å†™åœ¨ä¸€ä¸ªå…¬å¼é‡Œäº†ï¼Œåˆ†å¼€æ¥å†™å°±æ˜¯

- åˆ¤åˆ«å™¨éœ€è¦æœ€å°åŒ–å¦‚ä¸‹å‡½æ•°ï¼ˆå…¶å®å°±æ˜¯ä¸¤ä¸ªäº¤å‰ç†µæŸå¤±ç›¸åŠ ï¼‰

$$
-\mathbb{E}_{\boldsymbol{x} \sim p_{\text {data }}(\boldsymbol{x})}[\log D(\boldsymbol{x})]-\mathbb{E}_{\boldsymbol{x} \sim p_{\text{g}}(\boldsymbol{x})}[\log (1-D(\boldsymbol{x}))]
$$

- ç”Ÿæˆå™¨éœ€è¦æœ€å°åŒ–å¦‚ä¸‹å‡½æ•°
  $$
  \mathbb{E}_{\boldsymbol{x} \sim p_{\text{g}}(\boldsymbol{x})}[\log (1-D(\boldsymbol{x}))]
  $$
  Early in learning, when $G$ is poor, $D$ can reject samples with high confidence because they are clearly different from the training data. In this case, $\log(1 âˆ’ D(G(z)))$ saturates. Rather than training $G$ to minimize $\log(1 âˆ’ D(G(z)))$ we can train $G$ to maximize $\log D(G(z))$.

  <img src="GAN.assets/image-20210710164650446.png" alt="image-20210710164650446" style="zoom:50%;" />

  è®­ç»ƒ$G$â€‹çš„æ—¶å€™å°†æœ€å°åŒ–$\mathbb{E}_{\boldsymbol{x} \sim p_{\text{g}}(\boldsymbol{x})}[\log (1-D(\boldsymbol{x}))]$â€‹æ”¹ä¸ºæœ€å°åŒ–$\mathbb{E}_{\boldsymbol{x} \sim p_{\text{g}}(\boldsymbol{x})}[-\log (D(\boldsymbol{x}))]$â€‹ï¼Œä¹Ÿå°±æ˜¯äº¤å‰ç†µæŸå¤±

##### Train

![image-20210710165453372](GAN.assets/image-20210710165453372.png)

##### Optimal discriminator $D$ for any given generator $G$

**Proposition 1.** For $G$ fixed, the optimal discriminator $D$ is
$$
D_{G}^{*}(\boldsymbol{x})=\frac{p_{\text {data }}(\boldsymbol{x})}{p_{\text {data }}(\boldsymbol{x})+p_{g}(\boldsymbol{x})}
$$
**Proof.** The training criterion for the discriminator $D$, given any generator $G$, is to maximize the quantity $V(G, D)$
$$
\begin{aligned}
V(G, D) &=\int_{\boldsymbol{x}} p_{\text {data }}(\boldsymbol{x}) \log (D(\boldsymbol{x})) d x+\int_{z} p_{\boldsymbol{z}}(\boldsymbol{z}) \log (1-D(g(\boldsymbol{z}))) d z \\
&=\int_{\boldsymbol{x}} p_{\text {data }}(\boldsymbol{x}) \log (D(\boldsymbol{x}))+p_{g}(\boldsymbol{x}) \log (1-D(\boldsymbol{x})) d x
\end{aligned}
$$
å› ä¸ºæˆ‘ä»¬è¦æ‰¾çš„æ˜¯ä¸€ä¸ªæœ€ä¼˜åˆ¤åˆ«å™¨$D$ï¼Œæ­¤æ—¶$p_{data}(x)$å’Œ$p_g(x)$æ˜¯å›ºå®šçš„ï¼Œå¹¶åˆ†åˆ«ç”¨å¸¸æ•°$a$ï¼Œ$b$è¡¨ç¤ºï¼Œå‡½æ•°$D$ä¸æ˜¯å›ºå®šçš„ï¼Œæˆ‘ä»¬è¦æ‰¾çš„æ˜¯ä¸€ä¸ªèƒ½ä½¿$V$çš„å€¼æœ€å¤§çš„å‡½æ•°$D$ï¼Œ$x$è¾“å…¥$D$å¯ä»¥å¾—åˆ°ä»»æ„å€¼ï¼Œæ‰€ä»¥ç”¨$y$è¡¨ç¤º$D(x)$â€‹

For any $(a, b) \in \mathbb{R}^{2} \backslash\{0,0\}$, the function $y \rightarrow a \log (y)+b \log (1-y)$ achieves its maximum in
$[0,1]$ at $\frac{a}{a+b}$. The discriminator does not need to be defined outside of $\operatorname{Supp}\left(p_{\text {data }}\right) \cup \operatorname{Supp}\left(p_{g}\right)$, concluding the proof.

å³
$$
\begin{aligned}
C(G) &=\max _{D} V(G, D) \\
&=\mathbb{E}_{\boldsymbol{x} \sim p_{\text {data }}}\left[\log D_{G}^{*}(\boldsymbol{x})\right]+\mathbb{E}_{\boldsymbol{z} \sim p_{\boldsymbol{z}}}\left[\log \left(1-D_{G}^{*}(G(\boldsymbol{z}))\right)\right] \\
&=\mathbb{E}_{\boldsymbol{x} \sim p_{\text {data }}}\left[\log D_{G}^{*}(\boldsymbol{x})\right]+\mathbb{E}_{\boldsymbol{x} \sim p_{g}}\left[\log \left(1-D_{G}^{*}(\boldsymbol{x})\right)\right] \\
&=\mathbb{E}_{\boldsymbol{x} \sim p_{\text {data }}}\left[\log \frac{p_{\text {data }}(\boldsymbol{x})}{P_{\text {data }}(\boldsymbol{x})+p_{g}(\boldsymbol{x})}\right]+\mathbb{E}_{\boldsymbol{x} \sim p_{g}}\left[\log \frac{p_{g}(\boldsymbol{x})}{p_{\text {data }}(\boldsymbol{x})+p_{g}(\boldsymbol{x})}\right]
\end{aligned}
$$

##### Optimal generator $G$ for optimized $D$

**Theorem 1.** The global minimum of the virtual training criterion $C(G)$ is achieved if and only if $p_g = p_{data}$. At that point, $C(G)$ achieves the value $âˆ’\log4$.

**Proof.**
$$
\begin{aligned}
C(G) &=\mathbb{E}_{\boldsymbol{x} \sim p_{\text {data }}}\left[\log \frac{p_{\text {data }}(\boldsymbol{x})}{p_{\text {data }}(\boldsymbol{x})+p_{g}(\boldsymbol{x})}\right]+\mathbb{E}_{\boldsymbol{x} \sim p_{g}}\left[\log \frac{p_{g}(\boldsymbol{x})}{p_{\text {data }}(\boldsymbol{x})+p_{g}(\boldsymbol{x})}\right] \\
&=\mathbb{E}_{\boldsymbol{x} \sim p_{\text {data }}}\left[\log \frac{\frac{1}{2} * p_{\text {data }}(\boldsymbol{x})}{\frac{1}{2} *\left(p_{\text{data}}(\boldsymbol{x})+p_{g}(\boldsymbol{x})\right)}\right]+\mathbb{E}_{\boldsymbol{x} \sim p_{g}}\left[\log \frac{\frac{1}{2} * p_{g}(\boldsymbol{x})}{\frac{1}{2} *\left(p_{\text{data}}(\boldsymbol{x})+p_{g}(\boldsymbol{x})\right)}\right] \\
&=\mathbb{E}_{\boldsymbol{x} \sim p_{\text{data}}}\left[\log \frac{p_{\text{data}}(\boldsymbol{x})}{\frac{1}{2} *\left(p_{\text{data}}(\boldsymbol{x})+p_{g}(\boldsymbol{x})\right)}-\log 2\right]+\mathbb{E}_{\boldsymbol{x} \sim p_{g}}\left[\log \frac{p_{g}(\boldsymbol{x})}{\frac{1}{2}*\left(p_{\text{data}}(\boldsymbol{x})+p_{g}(\boldsymbol{x})\right)}-\log 2\right] \\
&=KL\left(p_{\text {data }} \| \frac{p_{\text {data }}+p_{g}}{2}\right)+K L\left(p_{g} \| \frac{p_{\text {data }}+p_{g}}{2}\right)-2\log2 \\
&=2*JS\left(p_{\text{data}}\|p_{g}\right)-2\log2
\end{aligned}
$$
å¯¹äºMinimax GANæŸå¤±å‡½æ•°ï¼Œå½“$p_{g}$çš„åˆ†å¸ƒç­‰äº$p_{data}$æ—¶ï¼Œ$C(G)$è¾¾åˆ°æœ€å°ï¼Œä¸º$-log4$

æ‰€ä»¥ï¼Œ$\max\limits_D V(G, D)$â€‹å®é™…ä¸Šæ˜¯åœ¨æ±‚ä¸¤ä¸ªåˆ†å¸ƒçš„JSæ•£åº¦ï¼Œè€Œ$\min\limits_G\max\limits_D V(G, D)$â€‹â€‹æ˜¯åœ¨æœ€å°åŒ–ä¸¤ä¸ªåˆ†å¸ƒé—´çš„JSæ•£åº¦ã€‚ï¼ˆæ³¨æ„ï¼šè¿™äº›æ•£åº¦æˆ‘ä»¬æ˜¯æ²¡æœ‰åŠæ³•é€šè¿‡å…¬å¼ç›´æ¥ç®—å‡ºæ¥çš„ï¼Œä½†æˆ‘ä»¬å¯ä»¥é€šè¿‡åˆ¤åˆ«å™¨ç½‘ç»œæ¥é—´æ¥è¡¡é‡ï¼‰

>*KLæ•£åº¦ä¸JSæ•£åº¦çš„å®šä¹‰*
>
><img src="GAN.assets/image-20210711204914070.png" alt="image-20210711204914070" style="zoom: 80%;" />
>
>KLæ•£åº¦ä¸å¯¹ç§°ï¼ŒKLæ•£åº¦$KL(p||q)âˆˆ[0,+\infin]$ï¼›JSæ•£åº¦å¯¹ç§°ï¼ˆ$p$ï¼Œ$q$äº’æ¢å€¼ä¸å˜ï¼‰ï¼ŒJSæ•£åº¦$JS(ğ‘ || q) âˆˆ [0, \log2]$ä¸ºæœ‰ç•Œå‡½æ•°
>

*ä¸ºä»€ä¹ˆåœ¨æ¯ä¸€è½®è®­ç»ƒä¸­$G$â€‹ä¸èƒ½è®­ç»ƒå¤ªå¤šï¼Ÿ*

ç”Ÿæˆå™¨$G$çš„ä¼˜åŒ–å‡½æ•°æ˜¯åœ¨æœ€ä¼˜åˆ¤åˆ«å™¨çš„æƒ…å†µä¸‹å¾—åˆ°çš„ï¼Œå½“$G$è®­ç»ƒçš„å¤ªå¼ºæ—¶æ­¤æ—¶çš„åˆ¤åˆ«å™¨å°±ä¸æ˜¯æœ€ä¼˜åˆ¤åˆ«å™¨äº†ï¼Œæ­¤æ—¶å»ä¼˜åŒ–è¿™ä¸ªç›®æ ‡å‡½æ•°ä¹Ÿå°±ä¸æ˜¯åœ¨ä¼˜åŒ–ä¸¤ä¸ªåˆ†å¸ƒä¹‹é—´çš„JSæ•£åº¦äº†ã€‚æ‰€ä»¥ä¸ºäº†è®©ç›®æ ‡å‡½æ•°å°½é‡ä¿æŒä¸å˜ï¼Œåº”è¯¥è®©Gä¸è¦è®­ç»ƒå¤ªå¤š

##### Non-saturating generator  loss

å¯¹äºNon-saturating GANï¼Œæˆ‘ä»¬æŠŠç”Ÿæˆå™¨çš„æŸå¤±å‡½æ•°$\mathbb{E}_{\boldsymbol{x} \sim p_{\text{g}}(\boldsymbol{x})}[\log (1-D(\boldsymbol{x}))]$æ”¹ä¸º$\mathbb{E}_{\boldsymbol{x} \sim p_{\text{g}}(\boldsymbol{x})}[-\log (D(\boldsymbol{x}))]$

The non-saturating game is heuristic, not being motivated by theory. However, the non-saturating game has other problems such as unstable numerical gradient for training $G$.

æ ¹æ®ä¸Šé¢çš„å…¬å¼ï¼Œæˆ‘ä»¬æœ‰
$$
\mathbb{E}_{x \sim p_{g}}\left[\log \left(1-D_{G}^{*}(x)\right)\right]=2*JS\left(p_{\text{data}}\|p_{g}\right)-2\log2-\mathbb{E}_{x \sim p_{\text {data }}}\left[\log D_{G}^{*}(x)\right]
$$
æœ€åä¸¤é¡¹ä¸ç”Ÿæˆå™¨æ— å…³ï¼Œæ‰€ä»¥å®é™…ä¸Šæ˜¯åœ¨æœ€å°åŒ–JSæ•£åº¦ï¼Œåˆå› ä¸º
$$
\begin{aligned}
&\mathbb{E}_{x \sim p_{g}}\left[-\log \left(D_{G}^{*}(x)\right)\right]+\mathbb{E}_{x \sim p_{g}}\left[\log \left(1-D_{G}^{*}(x)\right)\right] \\
&=\mathbb{E}_{x \sim p_{g}}\left[\log \frac{\left(1-D_{G}^{*}(x)\right)}{D_{G}^{*}(x)}\right]=\mathbb{E}_{x \sim p_{g}}\left[\log \frac{p_{g}(x)}{p_{\text {data }}(x)}\right] \\
&=K L\left(p_{g} \| p_{\text {data }}\right)
\end{aligned}
$$
æ‰€ä»¥
$$
\begin{aligned}
&\mathbb{E}_{x \sim p_{g}}\left[-\log \left(D_{G}^{*}(x)\right)\right] \\
&=K L\left(p_{g} \| p_{\text {data }}\right)-2*JS\left(p_{\text{data}}\|p_{g}\right)+2\log2+\mathbb{E}_{x \sim p_{\text {data }}}\left[\log D_{G}^{*}(x)\right]
\end{aligned}
$$
æœ€åä¸¤é¡¹ä¸ G æ— å…³ä¸ç”¨ç®¡ï¼Œå¯¹éé¥±å’Œåšå¼ˆä¸­çš„æ›¿ä»£ G æŸå¤±å‡½æ•°çš„ä¼˜åŒ–æ˜¯çŸ›ç›¾çš„ï¼Œå› ä¸ºç¬¬ä¸€é¡¹ç›®æ ‡æ˜¯ä½¿ç”Ÿæˆçš„åˆ†å¸ƒä¸å®é™…åˆ†å¸ƒä¹‹é—´çš„å·®å¼‚å°½å¯èƒ½å°ï¼Œè€Œç”±äºè´Ÿå·çš„å­˜åœ¨ï¼Œç¬¬äºŒé¡¹ç›®æ ‡æ˜¯ä½¿å¾—è¿™ä¸¤ä¸ªåˆ†å¸ƒä¹‹é—´çš„å·®å¼‚å°½å¯èƒ½å¤§ã€‚è¿™å°†ä¸ºè®­ç»ƒ G å¸¦æ¥ä¸ç¨³å®šçš„æ•°å€¼æ¢¯åº¦ã€‚

å…¶ä¸­JSæ•£åº¦$JS(ğ‘_g , p_{data} ) âˆˆ [0, \log 2]$â€‹ä¸ºæœ‰ç•Œå‡½æ•°ï¼Œå› æ­¤ç”Ÿæˆç½‘ç»œçš„ç›®æ ‡æ›´å¤šçš„æ˜¯å—é€†å‘KLæ•£åº¦$KL(ğ‘_g||ğ‘_{data})$â€‹å½±å“ï¼Œä½¿å¾—ç”Ÿæˆç½‘ç»œæ›´å€¾å‘äºç”Ÿæˆä¸€äº›æ›´â€œå®‰å…¨â€çš„æ ·æœ¬ï¼Œ ä»è€Œé€ æˆæ¨¡å‹åå¡Œï¼ˆModel Collapseï¼‰é—®é¢˜ã€‚

æ­¤å¤–ï¼Œæˆ‘ä»¬çŸ¥é“ KL ä¸æ˜¯å¯¹ç§°çš„ï¼Œå¯¹äºç”Ÿæˆå™¨æ— æ³•ç”ŸæˆçœŸå®æ ·æœ¬çš„æƒ…å†µï¼ŒKL å¯¹ loss çš„è´¡çŒ®éå¸¸å¤§ï¼Œè€Œå¯¹äºç”Ÿæˆå™¨ç”Ÿæˆçš„æ ·æœ¬å¤šæ ·æ€§ä¸è¶³çš„æ—¶å€™ï¼ŒKL å¯¹ loss çš„è´¡çŒ®éå¸¸å°ã€‚

![è®­ç»ƒGANï¼Œä½ åº”è¯¥çŸ¥é“çš„äºŒä¸‰äº‹](https://p6-tt.byteimg.com/origin/pgc-image/154540ce81bd45c699b699ea84329d00.png?from=pc)

##### Model Collapse

![image-20210725200959850](GAN.assets/image-20210725200959850.png)

##### Mode dropping

![image-20210725095037320](GAN.assets/image-20210725095037320.png)

å¦‚ä¸Šå›¾ï¼Œ$P_{data}$â€‹æ˜¯8ä¸ªé«˜æ–¯åˆ†å¸ƒçš„ç‚¹ï¼Œä¹Ÿå°±æ˜¯8ä¸ªmodeã€‚
æˆ‘ä»¬å¸Œæœ›ç»™å®šä¸€ä¸ªéšæœºé«˜æ–¯åˆ†å¸ƒï¼Œæˆ‘ä»¬å¸Œæœ›è¿™ä¸€ä¸ªéšæœºé«˜æ–¯åˆ†å¸ƒç»è¿‡Gæœ€åå¯ä»¥æ˜ å°„åˆ°è¿™8ä¸ªé«˜æ–¯åˆ†å¸ƒçš„modeä¸Šé¢å»ã€‚ä½†æ˜¯ä¸Šé¢ç¬¬äºŒè¡Œçš„å›¾è¡¨æ˜ï¼Œæˆ‘ä»¬ä¸èƒ½æ˜ å°„åˆ°è¿™8ä¸ªé«˜æ–¯åˆ†å¸ƒçš„modeä¸Šé¢ï¼Œæ•´ä¸ªGåªèƒ½ç”ŸæˆåŒä¸€ä¸ªmodeï¼Œç”±äºGå’ŒDçš„å¯¹æŠ—å…³ç³»ï¼ŒGä¸æ–­åˆ‡æ¢mode

- åœ¨step15kçš„æ—¶å€™ï¼ŒGçš„ä½ç½®åœ¨æŸä¸€ä¸ªGaussianæ‰€åœ¨ä½ç½®ï¼Œç„¶åDå‘ç°Gåªæ˜¯åœ¨è¿™ä¸ªGaussianè¿™é‡Œäº†ï¼Œæ‰€ä»¥å°±æŠŠè¿™ä¸ªåœ°æ–¹çš„æ‰€æœ‰data(æ— è®ºrealè¿˜æ˜¯fake)éƒ½ç»™åˆ¤å®šä¸ºfake
- Gå‘ç°åœ¨è¿™ä¸ªGaussianå¾…ä¸ä¸‹å»äº†ï¼Œåªä¼šè¢«Dæ°¸è¿œåˆ¤å®šä¸ºfakeï¼ˆå› ä¸ºè®­ç»ƒDçš„è´Ÿæ ·æœ¬éƒ½æ˜¯Gç”Ÿæˆçš„è¿™ç±»ç›¸ä¼¼çš„æ ·æœ¬ï¼Œæ‰€ä»¥è®­ç»ƒå‡ºæ¥çš„Dä¼šæŠŠè¿™ç±»æ ·æœ¬åˆ¤ä¸ºè´Ÿç±»ï¼‰ï¼Œæ‰€ä»¥å°±æƒ³ç€æ¢åˆ°å¦ä¸€ä¸ªåœ°æ–¹ã€‚åœ¨step25kå°±è·³åˆ°äº†å¦ä¸€ä¸ªGaussianä¸Šå»
- ç„¶åä¸æ–­è·³è·³è·³ï¼Œä¸æ–­é‡å¤ä¸Šè¿°ä¸¤ä¸ªè¿‡ç¨‹ï¼Œå°±åƒçŒ«æ‰è€é¼ çš„è¿‡ç¨‹ä¸€æ ·ï¼Œç„¶åå°±æ²¡æœ‰åŠæ³•åœä¸‹æ¥ï¼Œæ²¡æ³•è¾¾åˆ°æˆ‘ä»¬ç†æƒ³ä¸­æ˜ å°„åˆ°8ä¸ªä¸åŒçš„Gaussianä¸Šé¢å»

---

#### f-GAN

##### The f-divergence Family

$$
D_{f}(P \| Q) =\int_{x} q(x) f\left(\frac{p(x)}{q(x)}\right) d x
$$

where the generator function $f : \mathbb{R}_+ â†’ \mathbb{R}$â€‹ is a convex, lower-semicontinuous function satisfying $f(1) = 0$â€‹.

fæ˜¯ä¸€ä¸ªå‡¸å‡½æ•°åŒæ—¶ f(1)=0ï¼Œå¹¶ä¸” 0 æ˜¯$D_ğ‘“$èƒ½å–åˆ°çš„æœ€å°å€¼ï¼š
$$
D_{f}(P\|Q)=\int_{x} q(x) f\left(\frac{p(x)}{q(x)}\right)dx\ge f\left(\int_{x}q(x)\frac{p(x)}{q(x)}dx\right)=f(1)=0
$$

##### Fenchel Conjugate

Every convex, lower-semicontinuous function $f$ has a convex conjugate function $f^âˆ—$ , also known as Fenchel conjugate.
$$
f^{*}(t)=\max _{x \in \operatorname{dom}(f)}\{x t-f(x)\} \quad \Leftrightarrow \quad f(x)=\max _{t \in \operatorname{dom}\left(f^{*}\right)}\left\{x t-f^{*}(t)\right\}
$$
The function $f^âˆ—$ is again convex and lower-semicontinuous and the pair $(f, f^âˆ—)$ is dual to another in the sense that $f^{âˆ—âˆ—} = f$â€‹.

![image-20210725154427386](GAN.assets/image-20210725154427386.png)

##### f-Div GAN

$$
\begin{aligned}
D_{f}(P \| Q) &=\int_{x} q(x) f\left(\frac{p(x)}{q(x)}\right) d x \\
&=\int_{x} q(x)\left(\max _{t \in d o m\left(f^{*}\right)}\left\{\frac{p(x)}{q(x)} t-f^{*}(t)\right\}\right) d x \\
& \geqslant \int_{x} q(x)\left(\frac{p(x)}{q(x)} D(x)-f^{*}(D(x))\right) d x \\
&=\int_{x} p(x) D(x) d x-\int_{x} q(x) f^{*}(D(x)) d x \\
\end{aligned}
$$

ç¬¬ä¸‰è¡Œå°† $t$ æ›¿æ¢æˆ $D(x)$ å¹¶å°† $=$ æ›¿æ¢æˆ $â©¾$ åŸå› æ˜¯ï¼šæˆ‘ä»¬è¦æ±‚å¾—çš„æ˜¯ç»™å®š $x$ æ‰¾åˆ°ä¸€ ä¸ª $t$ ä½¿å¾—å¼å­æœ€å¤§ï¼Œä¹Ÿå°±æ˜¯è¯´ä¸ç®¡ $D(x)$â€‹ å–ä»€ä¹ˆå€¼éƒ½ä¸€å®šå°äºæˆ–è€…ç­‰äºç¬¬äºŒè¡Œçš„å¼å­ï¼›

æˆ‘è¦æ‰¾åˆ°ä¸€ä¸ª $D$ ä½¿å¾—ä¸Šé¢æœ€åä¸€æ­¥ä¸­çš„å¼å­æœ€å¤§ï¼Œä¸Šç•Œå°±æ˜¯ç­‰äºç¬¬äºŒè¡Œçš„å¼å­ï¼Œå³
$$
\begin{aligned}
D_{f}(P \| Q) &\approx \max _{D} \int_{x} p(x) D(x) d x-\int_{x} q(x) f^{*}(D(x)) d x\\
&=\max_{D}\{\mathbb{E}_{x\sim P}[D(x)]-\mathbb{E}_{x\sim Q}[f^*(D(x))]\}
\end{aligned}
$$
![image-20210725170352488](GAN.assets/image-20210725170352488.png)

å¯¹äºç”Ÿæˆå™¨æ¥è¯´ï¼Œæˆ‘ä»¬å°±æ˜¯è¦æ‰¾åˆ°ä¸€ä¸ª $ğ‘ƒ_ğº$ä½¿å¾—:
$$
\begin{aligned}
G^{*} &=\arg \min _{G} D_{f}\left(P_{\text {data }}|| P_{G}\right) \\
&=\arg \min _{G} \max _{D}\left\{E_{x \sim P_{\text {data }}}[D(x)]-E_{x \sim P_{G}}\left[f^{*}(D(x))\right]\right\} \\
&=\arg \min _{G} \max _{D} V(G, D)
\end{aligned}
$$
*f-GAN å‘Šè¯‰æˆ‘ä»¬çš„å°±æ˜¯ä¸€å¥è¯ï¼šä¸åªæ˜¯ JS Divï¼Œä»»ä½•çš„ Divï¼ˆç»Ÿç§°ä¸º f-Divï¼‰éƒ½å¯ä»¥è¢«æ”¾åˆ° GANs çš„æ¶æ„ä¸­å»ã€‚*

---

#### Improved  GAN

##### Semi-supervised learning

We can do semi-supervised learning with any standard classifier by simply adding samples from the GAN generator $G$ to our data set, labeling them with a new â€œgeneratedâ€ class $y = K + 1$, and correspondingly increasing the dimension of our classifier output from $K$ to $K + 1$. We may then use $p_{model}(y = K + 1 | x)$ to supply the probability that $x$ is fake, corresponding to $1 âˆ’ D(x)$ in the original GAN framework. We can now also learn from unlabeled data, as long as we know that it corresponds to one of the $K$ classes of real data by maximizing $\log p_{model}(y âˆˆ {1, . . . , K}|x)$. Assuming half of our data set consists of real data and half of it is generated (this is arbitrary), our loss function for training the classifier then becomes
$$
L=L_{supervised}+L_{unsupervised}
$$

![image-20210712211833858](GAN.assets/image-20210712211833858.png)

![image-20210712212411756](GAN.assets/image-20210712212411756.png)

![image-20210712212836899](GAN.assets/image-20210712212836899.png)

æ³¨æ„å…¬å¼ä¸­çš„$l_j=l_j-l_{K+1}$

![image-20210712213426864](GAN.assets/image-20210712213426864.png)

[è¯¦è§è¿™é‡Œ](https://blog.csdn.net/shenxiaolu1984/article/details/75736407)

---

#### W-GAN

##### JS Divä¸æ˜¯æœ€ä½³çš„Div

*ç”¨JSæ•£åº¦ä½œä¸ºæŸå¤±å‡½æ•°æœ‰ä»€ä¹ˆé—®é¢˜ï¼Ÿ*

æˆ‘ä»¬å…ˆè€ƒè™‘ä¸€ä¸‹$p_g$â€‹â€‹â€‹ ä¸ $p_{data}$â€‹â€‹â€‹çš„å®é™…åˆ†å¸ƒæƒ…å†µï¼Œæˆ‘ä»¬ä¼šå‘ç°ï¼Œå¤§å¤šæ•°æƒ…å†µä¸‹ $p_g$â€‹â€‹â€‹  ä¸ $p_{data}$â€‹â€‹â€‹æ˜¯æ²¡æœ‰é‡åˆçš„ã€‚å› ä¸ºä¸€æ–¹é¢ï¼Œä»ç†è®ºä¸Šæ¥è¯´ï¼Œ $p_g$â€‹â€‹â€‹ ä¸ $p_{data}$â€‹â€‹â€‹éƒ½å±äºé«˜ç»´ç©ºé—´ä¸­çš„ä½ç»´æµå½¢ï¼ŒäºŒè€…å…·æœ‰é‡åˆçš„å¯èƒ½æ€§æ˜¯éå¸¸ä½çš„ï¼›å¦å¤–ä¸€æ–¹é¢ï¼Œä»å®é™…ä¸Šæ¥çœ‹ï¼Œå³ç®—$p_g$â€‹â€‹â€‹ ä¸ $p_{data}$â€‹â€‹â€‹çš„åˆ†å¸ƒæœ‰äº†é‡åˆåŒºåŸŸï¼ˆå¦‚ä¸‹å·¦å›¾ï¼‰ï¼Œä½†æ˜¯åœ¨å®é™…è®­ç»ƒä¸­æˆ‘ä»¬æ˜¯ä» $p_g$â€‹â€‹â€‹ ä¸ $p_{data}$â€‹â€‹â€‹ä¸­å–çš„é‡‡æ ·ï¼Œè¿™äº›é‡‡æ ·å½¢æˆçš„åˆ†å¸ƒå¾ˆæœ‰å¯èƒ½æ˜¯äº’ä¸ç›¸äº¤çš„(å¦‚ä¸‹å³å›¾)ï¼Œæˆ‘ä»¬ä»ç„¶èƒ½æ‰¾åˆ°ä¸€æ¡åˆ†å‰²çº¿å°†$p_g$â€‹â€‹â€‹ ä¸ $p_{data}$â€‹â€‹â€‹å®Œç¾åˆ†å‰²å¼€æ¥ï¼ˆå¦‚å³å›¾ä¸­çš„é»‘çº¿ï¼‰ã€‚æ‰€ä»¥æˆ‘ä»¬å¯ä»¥è®¤ä¸ºï¼Œå¤§å¤šæ•°æƒ…å†µä¸‹ $p_g$â€‹â€‹â€‹ ä¸ $p_{data}$â€‹â€‹â€‹æ˜¯æ²¡æœ‰é‡åˆçš„ã€‚æ­¤å¤–ï¼Œä»ç›´è§‰çš„è§’åº¦æ¥çœ‹ï¼Œå‰é¢æœ‰æåˆ°JS Divæ˜¯é€šè¿‡åˆ¤åˆ«å™¨è®¡ç®—å‡ºæ¥çš„ï¼Œè€Œåˆ¤åˆ«å™¨çš„æœ¬è´¨æ˜¯äºŒåˆ†ç±»å™¨ï¼Œåªè¦$p_g$â€‹â€‹â€‹ä¸$p_{data}$â€‹â€‹å®Œå…¨æ²¡æœ‰é‡åˆï¼Œæœ€ä¼˜åˆ¤åˆ«å™¨å°±èƒ½å¾ˆå®¹æ˜“åœ°é‰´åˆ«å‡º$p_g(x)$â€‹â€‹ä¸ $p_{data}$â€‹çš„å·®å¼‚ï¼Œå› æ­¤äºŒè€…çš„ JS Div å°±æ˜¯ä¸€æ ·çš„ï¼Œéƒ½æ˜¯$\log 2$â€‹ã€‚ï¼ˆè¿™ä¹Ÿè¯´æ˜äº†åœ¨æ¯ä¸€è½®çš„è®­ç»ƒä¸­åˆ¤åˆ«å™¨ä¹Ÿä¸èƒ½è®­ç»ƒçš„å¤ªå¥½ï¼‰

![image-20210725210130022](GAN.assets/image-20210725210130022.png)

è®­ç»ƒç”Ÿæˆå™¨çš„ç›®æ ‡æ˜¯æœ€å°åŒ–$P_g$å’Œ$P_{data}$ä¹‹é—´çš„æ•£åº¦ï¼Œå…ˆç”¨åˆ¤åˆ«å™¨é‡å‡ºæ•£åº¦ï¼Œä½†æ˜¯å¯¹ç”Ÿæˆå™¨æ¥è¯´$P_{g0}$å’Œ$P_{g1}$æ˜¯ä¸€æ ·çš„ï¼Œå› æ­¤ç”Ÿæˆå™¨ä¸ä¼šå°†$P_{g0}$æ›´æ–°ä¸º$P_{g1}$â€‹ã€‚ï¼ˆä¸ºä»€ä¹ˆ$P_{G0}$ä¸$P_{G1}$å’Œ$P_{data}$çš„JSæ•£åº¦ä¸€æ ·å°±è¯´æ˜è®­ç»ƒç”Ÿæˆå™¨æ—¶æ²¡æœ‰æ¢¯åº¦ï¼Œå› ä¸ºJSæ•£åº¦ä¸€æ ·æ‰€ä»¥lossæ˜¯ä¸€æ ·çš„æ‰€ä»¥æ²¡æœ‰æ¢¯åº¦ï¼Ÿåæ¨JSæ•£åº¦æ˜¯æ€ä¹ˆç®—çš„ï¼ŒJSæ•£åº¦æ˜¯æœ€å°åŒ–åˆ¤åˆ«å™¨çš„äº¤å‰ç†µæŸå¤±ç®—å‡ºæ¥çš„ï¼ŒJSæ•£åº¦ä¸€æ ·è¯´æ˜åˆ¤åˆ«å™¨çš„äº¤å‰ç†µæŸå¤±ä¸€æ ·ï¼ŒæŸå¤±ä¸€æ ·å°±è¯´æ˜æ²¡æœ‰æ¢¯åº¦ï¼Œä½†è¿™é‡Œè¦è®©JSæ•£åº¦æœ€å°æ˜¯è¦æœ€å°åŒ–ç”Ÿæˆå™¨çš„æŸå¤±å‡½æ•°å•Šï¼Œåˆ¤åˆ«å™¨æ²¡æœ‰æ¢¯åº¦å’Œç”Ÿæˆå™¨æ²¡æœ‰æ¢¯åº¦æ²¡å¿…ç„¶è”ç³»å§ï¼Ÿè¿˜æ˜¯å› ä¸ºåŸå§‹GANä¸­åˆ¤åˆ«å™¨çš„æŸå¤±å‡½æ•°å’Œç”Ÿæˆå™¨çš„æŸå¤±å‡½æ•°æ˜¯ä¸€æ ·çš„åªæ˜¯ç¬¦å·ç›¸åï¼Ÿæ²¡ææ‡‚ï¼ï¼‰

![image-20210725222018592](GAN.assets/image-20210725222018592.png)

~~å¦å¤–ä¸€ä¸ªç›´è§‚çš„æƒ³æ³•æ˜¯ï¼šå½“ä½ learnä¸€ä¸ªäºŒå…ƒåˆ†ç±»å™¨çš„è¯ï¼Œä¼šç»™è“è‰²çš„ç‚¹0åˆ†ï¼Œç»¿è‰²ç‚¹1åˆ†ï¼›outputæ˜¯sigmoidå‡½æ•°ï¼Œåœ¨æ¥è¿‘0æˆ–1çš„åœ°æ–¹ç‰¹åˆ«å¹³ï¼Œæˆ‘ä»¬æœŸå¾…çš„æ˜¯ä½ trainä¸€ä¸ªç”Ÿæˆå™¨ï¼Œå®ƒä¼šå¸¦é¢†è“è‰²çš„ç‚¹é¡ºç€æ¢¯åº¦å»ç§»åŠ¨åˆ†å¸ƒï¼Œä½†æ˜¯è“è‰²çš„ç‚¹å‡ ä¹æ˜¯ä¸åŠ¨çš„ï¼Œå› ä¸ºå®ƒçš„é™„è¿‘æ¢¯åº¦å‡ ä¹æ˜¯0~~

*ä¸ºä»€ä¹ˆä¸¤ä¸ªåˆ†å¸ƒå®Œå…¨æ²¡æœ‰é‡å çš„éƒ¨åˆ†ï¼Œæˆ–è€…å®ƒä»¬é‡å çš„éƒ¨åˆ†å¯å¿½ç•¥ï¼Œå®ƒä»¬çš„JSæ•£åº¦æ˜¯$\log2$*ï¼Ÿ

å› ä¸ºé‡‡æ ·ä»»æ„ä¸€ä¸ªxåªæœ‰å››ç§å¯èƒ½

- $p_{data}(x) \rightarrow 0$â€‹ ä¸” $p_g(x) \rightarrow 0$â€‹
- $p_{data}(x) \nrightarrow 0$â€‹ ä¸” $p_g(x) \nrightarrow 0$â€‹
- $p_{data}(x) \rightarrow0$ ä¸” $p_g(x) \nrightarrow 0$
- $p_{data}(x) \nrightarrow 0$ ä¸” $p_g(x) \rightarrow 0$

ç¬¬ä¸€ç§å¯¹è®¡ç®—JSæ•£åº¦æ— è´¡çŒ®ï¼ˆæ ·æœ¬ä¸å±äºè¿™ä¸¤ä¸ªåˆ†å¸ƒä¸­çš„ä»»ä½•ä¸€ä¸ªï¼Œæ²¡æœ‰åŠæ³•è¡¡é‡ä¸¤ä¸ªåˆ†å¸ƒçš„ç›¸ä¼¼åº¦ï¼‰ï¼Œç¬¬äºŒç§æƒ…å†µç”±äºè¿™é‡Œé‡å éƒ¨åˆ†å¯å¿½ç•¥æ‰€ä»¥ä¹Ÿæ— è´¡çŒ®ï¼Œç¬¬ä¸‰ç¬¬å››ç§æƒ…å†µä»£å…¥JSæ•£åº¦é¡¹å¾—åˆ°çš„éƒ½æ˜¯$\log2$ï¼ˆKLæ•£åº¦éƒ½æ˜¯$+\infin$ï¼‰â€‹â€‹â€‹â€‹ï¼Œè€Œè¿™å¯¹äºæ¢¯åº¦ä¸‹é™æ–¹æ³•æ„å‘³ç€â€”â€”æ¢¯åº¦ä¸º0ï¼

><img src="GAN.assets/image-20210711204914070.png" alt="image-20210711204914070" style="zoom: 80%;" />

æ€»ä¹‹åŸå§‹GANä¸ç¨³å®šçš„åŸå› å°±æ˜¯ï¼šåˆ¤åˆ«å™¨è®­ç»ƒå¾—å¤ªå¥½ï¼Œç”Ÿæˆå™¨æ¢¯åº¦æ¶ˆå¤±ï¼Œç”Ÿæˆå™¨lossé™ä¸ä¸‹å»ï¼›åˆ¤åˆ«å™¨è®­ç»ƒå¾—ä¸å¥½ï¼Œç”Ÿæˆå™¨æ¢¯åº¦ä¸å‡†ï¼Œå››å¤„ä¹±è·‘ã€‚åªæœ‰åˆ¤åˆ«å™¨è®­ç»ƒå¾—ä¸å¥½ä¸åæ‰è¡Œï¼Œä½†æ˜¯è¿™ä¸ªç«å€™åˆå¾ˆéš¾æŠŠæ¡ï¼Œç”šè‡³åœ¨åŒä¸€è½®è®­ç»ƒçš„å‰åä¸åŒé˜¶æ®µè¿™ä¸ªç«å€™éƒ½å¯èƒ½ä¸ä¸€æ ·ï¼Œæ‰€ä»¥GANæ‰é‚£ä¹ˆéš¾è®­ç»ƒ





---

#### LS GAN

Regular GANs hypothesize the discriminator as a classifier with the sigmoid cross entropy loss function. However, we found that this loss function may lead to the vanishing gradients problem during the learning process.

![image-20210726093250252](GAN.assets/image-20210726093250252.png)

~~è“è‰²æ ·æœ¬ç‚¹ï¼Œæ˜¯ç”Ÿæˆæ ·æœ¬ğ‘ƒğºï¼Œå®ƒä»¬çš„å¾—åˆ†ä¸º 0ï¼›ç»¿è‰²æ ·æœ¬ç‚¹ï¼Œæ˜¯çœŸå®æ ·æœ¬ğ‘ƒğ‘‘ğ‘ğ‘¡ğ‘ï¼Œå®ƒä»¬çš„å¾—åˆ† ä¸º 1ï¼›ğ‘ƒğºä¸ğ‘ƒğ‘‘ğ‘ğ‘¡ğ‘ä¹‹é—´å®Œå…¨æ²¡æœ‰äº¤é›†ã€‚å½“è½®åˆ°ç”Ÿæˆå™¨è®­ç»ƒçš„æ—¶å€™ï¼Œå®ƒå¸Œæœ›è“è‰²çš„ç‚¹èƒ½å¤Ÿå‘å³ç§»ï¼Œä½†æ˜¯å› ä¸ºå¯¹äºæ‰€æœ‰è“è‰²ç‚¹ï¼Œåˆ¤åˆ«å™¨è®¡ç®—å‡ºçš„ JS Div éƒ½æ˜¯ä¸€æ ·çš„ï¼Œ è¿™æ„å‘³ç€æ‰€æœ‰ç‚¹çš„æ¢¯åº¦éƒ½æ˜¯ 0ï¼Œäºæ˜¯åŸºäº gradient descent æ‰€æœ‰çš„ç”Ÿæˆæ ·æœ¬çš„ç‚¹éƒ½æ— æ³•ç§»åŠ¨äº†~~

ï¼ˆæ²¡ææ‡‚ï¼‰

---

#### Info GAN

[InfoGANï¼šInterpretable Representation Learning by Information Maximizing GANsè®ºæ–‡è§£è¯»_sdnuwjwçš„åšå®¢-CSDNåšå®¢](https://blog.csdn.net/sdnuwjw/article/details/83614977)
